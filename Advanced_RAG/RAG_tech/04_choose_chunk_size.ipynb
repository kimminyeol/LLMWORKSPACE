{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import random\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from dotenv import load_dotenv\n",
    "# from openai import OpenAI\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "import openai\n",
    "import time\n",
    "import os\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "documents = SimpleDirectoryReader(data_dir).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)\n",
    "# documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create evaluation questions and pick k out of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages/llama_index/core/evaluation/dataset_generation.py:200: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n",
      "/opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages/llama_index/core/evaluation/dataset_generation.py:296: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "num_eval_questions = 25\n",
    "\n",
    "eval_documents = documents[0:20] # 문서중 20개만 뽑기 \n",
    "data_generator = DatasetGenerator.from_documents(eval_documents) # 질문을 뽑아낼 수 있는 문서 객체 만듦\n",
    "eval_questions = data_generator.generate_questions_from_nodes() # 질문 목록 만들어냄 \n",
    "k_eval_questions = random.sample(eval_questions, num_eval_questions) # 25개만 뽑아냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define metrics evaluators and modify llama_index faithfullness evaluator prompt to rely on the context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Define and configure Settings directly\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "# Set up faithfulness evaluation with a custom prompt\n",
    "faithfulness_new_prompt_template = PromptTemplate(\"\"\"Please tell if a given piece of information is directly supported by the context.\n",
    "    You need to answer with either YES or NO.\n",
    "    Answer YES if any part of the context explicitly supports the information, even if most of the context is unrelated. If the context does not explicitly support the information, answer NO. Some examples are provided below.\n",
    "\n",
    "    Information: Apple pie is generally double-crusted.\n",
    "    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
    "    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "    Answer: YES\n",
    "\n",
    "    Information: Apple pies taste bad.\n",
    "    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
    "    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "    Answer: NO\n",
    "\n",
    "    Information: Paris is the capital of France.\n",
    "    Context: This document describes a day trip in Paris. You will visit famous landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n",
    "    Answer: NO\n",
    "\n",
    "    Information: {query_str}\n",
    "    Context: {context_str}\n",
    "    Answer:\n",
    "\"\"\")\n",
    "\n",
    "# Initialize faithfulness evaluator with the custom prompt template\n",
    "faithfulness_gpt4 = FaithfulnessEvaluator()\n",
    "faithfulness_gpt4.update_prompts({\"your_prompt_key\": faithfulness_new_prompt_template})\n",
    "\n",
    "# Initialize relevancy evaluator\n",
    "relevancy_gpt4 = RelevancyEvaluator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We will use GPT-4 for evaluating the responses\n",
    "# gpt4 = OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# # Define service context for GPT-4 for evaluation\n",
    "# service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
    "\n",
    "# # 답변이 문맥에 얼마나 충실한지? \n",
    "# faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
    "\n",
    "# # context_str과 query_str이 주어졌을 때, 정보가 문맥에 의해 명확하게 뒷받침되는지 평가하는 프롬프트 \n",
    "# faithfulness_new_prompt_template = PromptTemplate(\"\"\" Please tell if a given piece of information is directly supported by the context.\n",
    "#     You need to answer with either YES or NO.\n",
    "#     Answer YES if any part of the context explicitly supports the information, even if most of the context is unrelated. If the context does not explicitly support the information, answer NO. Some examples are provided below.\n",
    "\n",
    "#     Information: Apple pie is generally double-crusted.\n",
    "#     Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "#     Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
    "#     It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "#     Answer: YES\n",
    "\n",
    "#     Information: Apple pies taste bad.\n",
    "#     Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
    "#     Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
    "#     It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
    "#     Answer: NO\n",
    "\n",
    "#     Information: Paris is the capital of France.\n",
    "#     Context: This document describes a day trip in Paris. You will visit famous landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n",
    "#     Answer: NO\n",
    "\n",
    "#     Information: {query_str}\n",
    "#     Context: {context_str}\n",
    "#     Answer:\n",
    "\n",
    "#     \"\"\")\n",
    "\n",
    "# # 충실성 평가할 새로운 프롬프트 템플릿을 새로 설정함 \n",
    "# faithfulness_gpt4.update_prompts({\"your_prompt_key\": faithfulness_new_prompt_template}) \n",
    "# # 질문이 응답과 얼마나 관련이 높은지? \n",
    "# relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate metrics for each chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "# 평가 함수 정의\n",
    "def evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n",
    "    \"\"\"\n",
    "    주어진 청크 크기에 대해 GPT-3.5-turbo로 생성된 응답의 평균 응답 시간, 충실성, 관련성을 평가합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    chunk_size (int): 데이터 청크 크기\n",
    "    \n",
    "    Returns:\n",
    "    tuple: 평균 응답 시간, 충실성, 관련성 점수를 포함하는 튜플\n",
    "    \"\"\"\n",
    "\n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "\n",
    "    # gpt-3.5-turbo 모델을 직접 사용하여 응답 생성\n",
    "    def query_gpt_35(query):\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": query}]\n",
    "        )\n",
    "        return response.choices[0].message['content']\n",
    "\n",
    "    # 벡터 인덱스 설정 (ServiceContext 사용 없이)\n",
    "    # 여기에 vector_index와 query_engine을 설정하는 다른 방법을 넣어야 할 수 있습니다.\n",
    "    # vector_index = VectorStoreIndex.from_documents(eval_documents, chunk_size=chunk_size, chunk_overlap=chunk_size//5)\n",
    "    # query_engine = vector_index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "    num_questions = len(eval_questions)\n",
    "\n",
    "    # 각 질문에 대해 응답 생성 및 평가\n",
    "    for question in eval_questions:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # gpt-3.5-turbo로 응답 생성\n",
    "        response_vector = query_gpt_35(question)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # GPT-4로 충실성 평가\n",
    "        faithfulness_result = faithfulness_gpt4.evaluate_response(response=response_vector).passing\n",
    "        \n",
    "        # GPT-4로 관련성 평가\n",
    "        relevancy_result = relevancy_gpt4.evaluate_response(query=question, response=response_vector).passing\n",
    "\n",
    "        total_response_time += elapsed_time\n",
    "        total_faithfulness += faithfulness_result\n",
    "        total_relevancy += relevancy_result\n",
    "\n",
    "    # 평균값 계산\n",
    "    average_response_time = total_response_time / num_questions\n",
    "    average_faithfulness = total_faithfulness / num_questions\n",
    "    average_relevancy = total_relevancy / num_questions\n",
    "\n",
    "    return average_response_time, average_faithfulness, average_relevancy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different chunk sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_size \u001b[38;5;129;01min\u001b[39;00m chunk_sizes:\n\u001b[0;32m----> 4\u001b[0m   avg_response_time, avg_faithfulness, avg_relevancy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_response_time_and_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_eval_questions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Average Response time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_response_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, Average Faithfulness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_faithfulness\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Relevancy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_relevancy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 40\u001b[0m, in \u001b[0;36mevaluate_response_time_and_accuracy\u001b[0;34m(chunk_size, eval_questions)\u001b[0m\n\u001b[1;32m     37\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# gpt-3.5-turbo로 응답 생성\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m response_vector \u001b[38;5;241m=\u001b[39m \u001b[43mquery_gpt_35\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# GPT-4로 충실성 평가\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[49], line 22\u001b[0m, in \u001b[0;36mevaluate_response_time_and_accuracy.<locals>.query_gpt_35\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_gpt_35\u001b[39m(query):\n\u001b[0;32m---> 22\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = [128, 256]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, k_eval_questions)\n",
    "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from openai==0.28) (4.66.5)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from openai==0.28) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from aiohttp->openai==0.28) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from aiohttp->openai==0.28) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp->openai==0.28) (0.2.0)\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.52.1\n",
      "    Uninstalling openai-1.52.1:\n",
      "      Successfully uninstalled openai-1.52.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ragas 0.2.2 requires openai>1, but you have openai 0.28.0 which is incompatible.\n",
      "llama-index-agent-openai 0.3.4 requires openai>=1.14.0, but you have openai 0.28.0 which is incompatible.\n",
      "llama-index-legacy 0.9.48.post3 requires openai>=1.1.0, but you have openai 0.28.0 which is incompatible.\n",
      "langchain-openai 0.2.3 requires openai<2.0.0,>=1.52.0, but you have openai 0.28.0 which is incompatible.\n",
      "llama-index-embeddings-openai 0.2.5 requires openai>=1.1.0, but you have openai 0.28.0 which is incompatible.\n",
      "llama-index-llms-openai 0.2.16 requires openai<2.0.0,>=1.40.0, but you have openai 0.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.28.0\n"
     ]
    }
   ],
   "source": [
    "! pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
