{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR: 재귀적 추상적 처리 및 주제별 조직화(RAPTOR) 기반 검색 시스템\n",
    "\n",
    "## 개요\n",
    "RAPTOR는 계층적 문서 요약, 임베딩 기반 검색, 문맥적 응답 생성 기능을 결합한 고급 정보 검색 및 질문-응답 시스템입니다. 대규모 문서 집합을 효율적으로 처리하기 위해 다층 트리 구조의 요약본을 생성하여, 넓은 범위와 세부적인 정보 검색을 모두 가능하게 합니다.\n",
    "\n",
    "## 동기\n",
    "기존 검색 시스템은 대규모 문서 집합을 다루는 데 있어 중요한 세부 정보를 놓치거나 불필요한 정보로 인해 과부하가 발생하는 문제를 자주 겪습니다. RAPTOR는 문서 집합의 계층적 구조를 만들어 상위 개념과 특정 세부 정보를 상황에 맞게 탐색할 수 있도록 하여 이러한 문제를 해결합니다.\n",
    "\n",
    "## 주요 구성 요소\n",
    "1. **트리 빌딩**: 문서 요약의 계층적 구조 생성\n",
    "2. **임베딩 및 클러스터링**: 의미적 유사성을 기반으로 문서와 요약 정리\n",
    "3. **벡터 저장소**: 문서와 요약의 임베딩을 효율적으로 저장하고 검색\n",
    "4. **문맥적 검색기**: 특정 쿼리에 맞는 가장 관련성 높은 정보 선택\n",
    "5. **응답 생성**: 검색된 정보를 바탕으로 일관된 답변 생성\n",
    "\n",
    "## 방법 상세 설명\n",
    "\n",
    "### 트리 빌딩\n",
    "1. 레벨 0에서는 원본 문서로 시작합니다.\n",
    "2. 각 레벨에서 다음 과정을 수행합니다:\n",
    "   - 언어 모델을 사용하여 텍스트 임베딩 생성\n",
    "   - 임베딩 클러스터링(예: Gaussian Mixture Models 사용)\n",
    "   - 각 클러스터에 대해 요약본 생성\n",
    "   - 생성된 요약본을 다음 레벨의 텍스트로 사용\n",
    "3. 하나의 요약본에 도달하거나 최대 레벨에 도달할 때까지 반복\n",
    "\n",
    "### 임베딩 및 검색\n",
    "1. 트리의 모든 레벨의 문서와 요약본을 임베딩합니다.\n",
    "2. 이러한 임베딩을 벡터 저장소(예: FAISS)에 저장하여 유사성 검색을 빠르게 수행합니다.\n",
    "3. 쿼리가 주어지면:\n",
    "   - 쿼리를 임베딩합니다.\n",
    "   - 벡터 저장소에서 가장 유사한 문서/요약을 검색합니다.\n",
    "\n",
    "### 문맥적 압축\n",
    "1. 검색된 문서/요약본을 가져옵니다.\n",
    "2. 언어 모델을 사용하여 쿼리에 가장 관련성이 높은 부분만 추출합니다.\n",
    "\n",
    "### 응답 생성\n",
    "1. 관련된 부분을 결합하여 문맥을 구성합니다.\n",
    "2. 이 문맥과 원래의 쿼리를 바탕으로 언어 모델을 사용해 답변을 생성합니다.\n",
    "\n",
    "## 접근 방식의 장점\n",
    "1. **확장성**: 다양한 수준의 요약을 사용하여 대규모 문서 집합을 처리할 수 있습니다.\n",
    "2. **유연성**: 높은 수준의 개요와 특정 세부 정보를 모두 제공할 수 있습니다.\n",
    "3. **문맥 인식**: 추상화 수준에 따라 가장 적합한 정보를 검색합니다.\n",
    "4. **효율성**: 임베딩과 벡터 저장소를 활용하여 빠른 검색을 수행합니다.\n",
    "5. **추적 가능성**: 요약본과 원본 문서 간의 링크를 유지하여 출처 확인이 가능합니다.\n",
    "\n",
    "## 결론\n",
    "RAPTOR는 정보 검색 및 질문-응답 시스템에 있어서 중요한 진전을 나타냅니다. 계층적 요약과 임베딩 기반 검색 및 문맥적 응답 생성을 결합하여, 대규모 문서 집합을 효과적으로 다룰 수 있는 강력하고 유연한 접근 방식을 제공합니다. 이 시스템의 계층적 추상화 탐색 기능을 통해 폭넓은 쿼리에 대해 관련성 높고 문맥적으로 적합한 답변을 제공할 수 있습니다.\n",
    "\n",
    "RAPTOR는 앞으로 트리 빌딩 프로세스 최적화, 요약 품질 향상, 복잡하고 다면적인 쿼리를 더 잘 처리할 수 있는 검색 메커니즘 개선 등에 대한 추가 연구가 필요합니다. 또한, 다른 AI 기술과 결합함으로써 더욱 정교한 정보 처리 시스템을 개발할 수 있는 가능성도 열려 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/raptor.svg\" alt=\"RAPTOR\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gl/6qlsk6x94klgs54qzsg9q1jm0000gn/T/ipykernel_3933/1959881810.py:22: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from helper_functions import *\n",
      "/opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages/deepeval/__init__.py:49: UserWarning: You are using deepeval version 1.4.5, however version 1.4.8 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "from helper_functions import *\n",
    "from evaluation.evalute_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define logging, llm and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(item):\n",
    "    \"\"\"Extract text content from either a string or an AIMessage object.\"\"\"\n",
    "    # `item`이 AIMessage 객체인지 확인하고, 그렇다면 `content` 속성을 반환합니다.\n",
    "    # 그렇지 않으면 `item` 자체를 반환하여 일반 문자열도 처리할 수 있게 합니다.\n",
    "    if isinstance(item, AIMessage):\n",
    "        return item.content\n",
    "    return item\n",
    "\n",
    "# aimessage 텍스트에 대해 임베딩을 실시 \n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    logging.info(f\"Embedding {len(texts)} texts\")  # 로그 기록: 임베딩할 텍스트 수 표시\n",
    "    return embeddings.embed_documents([extract_text(text) for text in texts])\n",
    "\n",
    "# 주어진 임베딩 배열들에 대해 클러스터링함  -> 주어진 임베딩이 어느 클러스터에 속해있는지 레이블 반환 \n",
    "def perform_clustering(embeddings: np.ndarray, n_clusters: int = 10) -> np.ndarray:\n",
    "    logging.info(f\"Performing clustering with {n_clusters} clusters\")  # 클러스터 수 표시\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=42)  # GMM 객체 생성\n",
    "    return gm.fit_predict(embeddings)  # 클러스터링 수행 후, 각 데이터 포인트의 클러스터 레이블 반환\n",
    "\n",
    "# 입력된 텍스트 리스트를 요약함 \n",
    "def summarize_texts(texts: List[str]) -> str:\n",
    "    \"\"\"Summarize a list of texts using OpenAI.\"\"\"\n",
    "    logging.info(f\"Summarizing {len(texts)} texts\")  # 요약할 텍스트 수 표시\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Summarize the following text concisely:\\n\\n{text}\"\n",
    "    )\n",
    "    chain = prompt | llm  # 프롬프트와 LLM을 연결하여 체인 생성\n",
    "    input_data = {\"text\": texts}  # 입력 데이터 생성\n",
    "    return chain.invoke(input_data)  # 체인 호출하여 요약 결과 반환\n",
    "\n",
    "# pca로 2차원으로 임베딩을 축소하여 시각화함 \n",
    "def visualize_clusters(embeddings: np.ndarray, labels: np.ndarray, level: int):\n",
    "    from sklearn.decomposition import PCA  # PCA 클래스 임포트\n",
    "    pca = PCA(n_components=2)  # 2개의 주성분으로 차원 축소 설정\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)  # 임베딩 차원 축소\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))  # 플롯 크기 설정\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)  # 컬러 바 추가하여 각 클러스터 색상 구분\n",
    "    plt.title(f'Cluster Visualization - Level {level}')  # 제목에 클러스터 레벨 표시\n",
    "    plt.xlabel('First Principal Component')  # x축 레이블\n",
    "    plt.ylabel('Second Principal Component')  # y축 레이블\n",
    "    plt.show()  # 플롯 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPTOR Core Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_raptor_tree(texts: List[str], max_levels: int = 3) -> Dict[int, pd.DataFrame]:\n",
    "    # 각 레벨의 메타데이터와 부모-자식 관계를 포함하는 RAPTOR 트리 구조를 생성합니다.\n",
    "    results = {}  # 각 레벨별 데이터를 저장할 딕셔너리\n",
    "    current_texts = [extract_text(text) for text in texts]  # 초기 텍스트 리스트 생성\n",
    "    current_metadata = [{\"level\": 0, \"origin\": \"original\", \"parent_id\": None} for _ in texts]\n",
    "    # 각 텍스트에 대해 초기 메타데이터 설정 (레벨 0, 원본 데이터, 부모 없음)\n",
    "\n",
    "# 레벨 1부터 시작 \n",
    "    for level in range(1, max_levels + 1):\n",
    "        logging.info(f\"Processing level {level}\")  # 현재 레벨 \n",
    "\n",
    "        # 텍스트 임베딩 생성\n",
    "        embeddings = embed_texts(current_texts)\n",
    "        \n",
    "        # 클러스터 수 설정 (최소 10개, 또는 현재 텍스트 수의 절반 이하)\n",
    "        n_clusters = min(10, len(current_texts) // 2)\n",
    "        \n",
    "        # Gaussian Mixture Model을 사용하여 클러스터링 수행\n",
    "        cluster_labels = perform_clustering(np.array(embeddings), n_clusters)\n",
    "\n",
    "        # 현재 레벨의 데이터프레임 생성 및 결과 저장\n",
    "        df = pd.DataFrame({\n",
    "            'text': current_texts,\n",
    "            'embedding': embeddings,\n",
    "            'cluster': cluster_labels,\n",
    "            'metadata': current_metadata\n",
    "        })\n",
    "        results[level-1] = df  # 각 레벨의 결과 저장\n",
    "\n",
    "        summaries = []  # 요약본을 저장할 리스트\n",
    "        new_metadata = []  # 새 메타데이터 저장할 리스트\n",
    "\n",
    "        # 현재 레벨의 각 클러스터별 텍스트 추출, 메타데이터 추출  -> 각 클러스의 텍스트를 요약 후 리스트에 넣음 \n",
    "        for cluster in df['cluster'].unique():\n",
    "            # 각 클러스터에 대해 텍스트와 메타데이터를 추출하고 요약을 생성\n",
    "            cluster_docs = df[df['cluster'] == cluster]\n",
    "            cluster_texts = cluster_docs['text'].tolist()\n",
    "            cluster_metadata = cluster_docs['metadata'].tolist()\n",
    "            \n",
    "            # 클러스터 텍스트 요약 생성-> 각 레벨 별 모든 클러스터에 대한 텍스트의 요약을 리스트로 한다. -> 즉, 요약 하나에 메타데이터 하나 \n",
    "            summary = summarize_texts(cluster_texts)\n",
    "            summaries.append(summary)\n",
    "            \n",
    "            # 요약본에 대한 메타데이터 생성\n",
    "            new_metadata.append({\n",
    "                \"level\": level,\n",
    "                \"origin\": f\"summary_of_cluster_{cluster}_level_{level-1}\",\n",
    "                \"child_ids\": [meta.get('id') for meta in cluster_metadata], # 자식레벨에서 사용했던 id -> 클러스터링 된 것들 \n",
    "                \"id\": f\"summary_{level}_{cluster}\" # 현 부모 id-> 클러스터 대표 id \n",
    "            })\n",
    "\n",
    "        # 다음 레벨에 사용할 텍스트와 메타데이터 업데이트 -> 각 클러스터별 요약된 내용, 메타데이터가 새롭게 정의된다. \n",
    "        current_texts = summaries\n",
    "        current_metadata = new_metadata # 다음 메타데이터 \n",
    "\n",
    "        # 요약본이 하나만 남았을 경우 트리 생성 중단\n",
    "        if len(current_texts) <= 1:\n",
    "            results[level] = pd.DataFrame({\n",
    "                'text': current_texts,\n",
    "                'embedding': embed_texts(current_texts),\n",
    "                'cluster': [0],\n",
    "                'metadata': current_metadata\n",
    "            })\n",
    "            logging.info(f\"Stopping at level {level} as we have only one summary\")  # 중단 기록\n",
    "            break\n",
    "\n",
    "    return results  # 각 레벨의 데이터프레임을 포함한 최종 트리 구조 반환 -> 레벨 별 텍스트의 요약과 메타데이터와 그에 해당하는 임베딩 및 클러스터가 들어있음 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstore Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorstore(tree_results: Dict[int, pd.DataFrame]) -> FAISS:\n",
    "    \"\"\"Build a FAISS vectorstore from all texts in the RAPTOR tree.\"\"\"\n",
    "    all_texts = [] #\n",
    "    all_embeddings = []\n",
    "    all_metadatas = []\n",
    "    \n",
    "    for level, df in tree_results.items():\n",
    "        all_texts.extend([str(text) for text in df['text'].tolist()])  # 모든 텍스트를 저장 \n",
    "        all_embeddings.extend([embedding.tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in df['embedding'].tolist()]) # 모든 임베딩을 저장 \n",
    "        all_metadatas.extend(df['metadata'].tolist()) # 모든 메타데이터를 저장 \n",
    "    \n",
    "    logging.info(f\"Building vectorstore with {len(all_texts)} texts\")\n",
    "    \n",
    "    # 모든 문서와 메타데이터에 있는 내용과 메타데이터를 뽑아서 리스트화 \n",
    "    documents = [Document(page_content=str(text), metadata=metadata) \n",
    "                 for text, metadata in zip(all_texts, all_metadatas)]\n",
    "    \n",
    "    # 문서와 임베딩에 대해 FAISS에 저장 \n",
    "    return FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tree traversal retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최상위 레벨에서 시작하여 하위 라벨로 이동하면서 세부 정보를 검색 \n",
    "# 각 레벨에서 자식 ID(child_ids)를 추출하여 쿼리와 연관된 문서들만 하위 레벨로 넘깁니다.\n",
    "# 쿼리와 유사한 상위 요약본과 함께, 하위 레벨의 더 세부적인 정보를 포함한 문서들을 모두 반환합니다.\n",
    "\n",
    "\n",
    "def tree_traversal_retrieval(query: str, vectorstore: FAISS, k: int = 3) -> List[Document]:\n",
    "    # 입력된 쿼리에 대한 임베딩 생성\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    \n",
    "    # 특정 레벨에서 문서 검색\n",
    "    def retrieve_level(level: int, parent_ids: List[str] = None) -> List[Document]:\n",
    "        # 부모 ID가 주어졌을 경우, 해당 부모 ID와 레벨에 맞는 문서 검색\n",
    "        # 부모 레벨에서 유사한 문서들을 검색함 \n",
    "        if parent_ids:\n",
    "            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                query_embedding,\n",
    "                k=k,\n",
    "                filter=lambda meta: meta['level'] == level and meta['id'] in parent_ids\n",
    "            )\n",
    "        else:\n",
    "            # 부모 ID가 없는 경우, 현재 레벨에 해당하는 모든 문서 중 유사도 기반으로 검색\n",
    "            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                query_embedding,\n",
    "                k=k,\n",
    "                filter=lambda meta: meta['level'] == level\n",
    "            )\n",
    "        \n",
    "        # 현재 레벨에서 검색 결과가 없거나 레벨이 0이면 현재 문서를 반환\n",
    "        if not docs or level == 0:\n",
    "            return docs\n",
    "        \n",
    "        # 검색된 문서들에서 자식 ID 수집\n",
    "        child_ids = [doc.metadata.get('child_ids', []) for doc, _ in docs]\n",
    "        child_ids = [item for sublist in child_ids for item in sublist]  # 리스트 평탄화\n",
    "        \n",
    "        # 하위 레벨에서 자식 ID를 기반으로 재귀적으로 문서 검색\n",
    "        child_docs = retrieve_level(level - 1, child_ids)\n",
    "        return docs + child_docs  # 현재 레벨 문서와 하위 레벨 문서를 합쳐 반환\n",
    "    \n",
    "    # 트리의 최상위 레벨을 얻기 위해 벡터 저장소의 모든 문서 메타데이터에서 최대 레벨 찾기\n",
    "    max_level = max(doc.metadata['level'] for doc in vectorstore.docstore.values())\n",
    "    return retrieve_level(max_level)  # 최상위 레벨에서 검색 시작\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(vectorstore: FAISS) -> ContextualCompressionRetriever:\n",
    "    logging.info(\"Creating contextual compression retriever\")\n",
    "    base_retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    # 문서와 질문이 주어져 있을 때 질문에 답변하기 위해 관련된 정보만 뽑기 \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context and question, extract only the relevant information for answering the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Relevant Information:\"\n",
    "    )\n",
    "    \n",
    "    extractor = LLMChainExtractor.from_llm(llm, prompt=prompt)\n",
    "    \n",
    "    # 전체 문서에서 관련 문서를 찾고 찾은 문서에서 관련된 부분만 추출함\n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=extractor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hierarchical retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_retrieval(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> List[Document]:\n",
    "    \"\"\"Perform hierarchical retrieval starting from the highest level, handling potential None values.\"\"\"\n",
    "    all_retrieved_docs = []\n",
    "    \n",
    "    for level in range(max_level, -1, -1):\n",
    "        # 현재 레벨에서 쿼리에 맞는 문서만 추출함 \n",
    "        level_docs = retriever.get_relevant_documents(\n",
    "            query,\n",
    "            filter=lambda meta: meta['level'] == level\n",
    "        )\n",
    "        all_retrieved_docs.extend(level_docs)\n",
    "        \n",
    "        # 하위 레벨 검색을 위해 자식 id 수집 \n",
    "        if level_docs and level > 0:\n",
    "            child_ids = [doc.metadata.get('child_ids', []) for doc in level_docs]\n",
    "            child_ids = [item for sublist in child_ids for item in sublist if item is not None]  # Flatten and filter None\n",
    "            \n",
    "            # 자식 id가 있을 때 쿼리에 자식 쿼리를 합쳐서 전달함 \n",
    "            if child_ids: \n",
    "                child_query = f\" AND id:({' OR '.join(str(id) for id in child_ids)})\"\n",
    "                query += child_query\n",
    "    \n",
    "    return all_retrieved_docs # 최종 합친 문서 출력 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPTOR Query Process (Online Process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raptor_query(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> Dict[str, Any]:\n",
    "    \"\"\"Process a query using the RAPTOR system with hierarchical retrieval.\"\"\"\n",
    "    logging.info(f\"Processing query: {query}\")  # 쿼리 처리 시작 로그 기록\n",
    "    \n",
    "    # 1. 계층적 검색 수행\n",
    "    relevant_docs = hierarchical_retrieval(query, retriever, max_level)  # 주어진 쿼리에 맞는 문서 검색\n",
    "    \n",
    "    # 2. 검색된 문서의 세부 정보 저장\n",
    "    doc_details = []\n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        doc_details.append({\n",
    "            \"index\": i,  # 문서 인덱스\n",
    "            \"content\": doc.page_content,  # 문서의 내용\n",
    "            \"metadata\": doc.metadata,  # 메타데이터 (레벨, 출처 등)\n",
    "            \"level\": doc.metadata.get('level', 'Unknown'),  # 문서의 레벨 정보 (알 수 없는 경우 'Unknown')\n",
    "            \"similarity_score\": doc.metadata.get('score', 'N/A')  # 유사도 점수 (없으면 'N/A')\n",
    "        })\n",
    "    \n",
    "    # 3. 답변 생성에 사용할 컨텍스트 설정\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])  # 각 문서의 내용을 컨텍스트로 합침\n",
    "    \n",
    "    # 4. 답변 생성을 위한 프롬프트 설정\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context, please answer the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # 프롬프트에 따라 LLM과의 체인 설정\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    answer = chain.run(context=context, question=query)  # 설정한 체인으로 답변 생성\n",
    "    \n",
    "    logging.info(\"Query processing completed\")  # 쿼리 처리 완료 로그 기록\n",
    "    \n",
    "    # 5. 결과 반환\n",
    "    result = {\n",
    "        \"query\": query,  # 쿼리 내용\n",
    "        \"retrieved_documents\": doc_details,  # 검색된 문서들의 상세 정보\n",
    "        \"num_docs_retrieved\": len(relevant_docs),  # 검색된 문서의 개수\n",
    "        \"context_used\": context,  # 답변 생성에 사용된 전체 컨텍스트\n",
    "        \"answer\": answer,  # 생성된 답변\n",
    "        \"model_used\": llm.model_name,  # 사용된 모델 이름\n",
    "    }\n",
    "    \n",
    "    return result  # 최종 결과 반환\n",
    "\n",
    "\n",
    "def print_query_details(result: Dict[str, Any]):\n",
    "    \"\"\"Print detailed information about the query process, including tree level metadata.\"\"\"\n",
    "    print(f\"Query: {result['query']}\")  # 쿼리 출력\n",
    "    print(f\"\\nNumber of documents retrieved: {result['num_docs_retrieved']}\")  # 검색된 문서 개수 출력\n",
    "    print(f\"\\nRetrieved Documents:\")\n",
    "    \n",
    "    # 각 검색된 문서의 세부 정보 출력\n",
    "    for doc in result['retrieved_documents']:\n",
    "        print(f\"  Document {doc['index']}:\")\n",
    "        print(f\"    Content: {doc['content'][:100]}...\")  # 문서 내용의 첫 100자 출력\n",
    "        print(f\"    Similarity Score: {doc['similarity_score']}\")  # 유사도 점수 출력\n",
    "        print(f\"    Tree Level: {doc['metadata'].get('level', 'Unknown')}\")  # 문서 레벨 정보 출력\n",
    "        print(f\"    Origin: {doc['metadata'].get('origin', 'Unknown')}\")  # 문서의 출처 정보 출력\n",
    "        if 'child_docs' in doc['metadata']:\n",
    "            print(f\"    Number of Child Documents: {len(doc['metadata']['child_docs'])}\")  # 자식 문서 수 출력\n",
    "        print()  # 빈 줄 추가로 문서 구분\n",
    "    \n",
    "    # 답변 생성에 사용된 전체 컨텍스트 출력\n",
    "    print(f\"\\nContext used for answer generation:\")\n",
    "    print(result['context_used'])\n",
    "    \n",
    "    # 생성된 답변 출력\n",
    "    print(f\"\\nGenerated Answer:\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    # 사용된 모델 이름 출력\n",
    "    print(f\"\\nModel Used: {result['model_used']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage and Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()\n",
    "texts = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RAPTOR components instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 03:00:48,867 - INFO - Processing level 1\n",
      "2024-11-02 03:00:48,868 - INFO - Embedding 33 texts\n",
      "2024-11-02 03:00:50,102 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:00:51,386 - INFO - Performing clustering with 10 clusters\n",
      "2024-11-02 03:00:56,257 - INFO - Summarizing 1 texts\n",
      "2024-11-02 03:00:58,434 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:00:58,456 - INFO - Summarizing 2 texts\n",
      "2024-11-02 03:01:01,296 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:01,302 - INFO - Summarizing 3 texts\n",
      "2024-11-02 03:01:04,674 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:04,677 - INFO - Summarizing 4 texts\n",
      "2024-11-02 03:01:10,169 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:10,172 - INFO - Summarizing 9 texts\n",
      "2024-11-02 03:01:12,292 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:12,299 - INFO - Summarizing 4 texts\n",
      "2024-11-02 03:01:16,344 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:16,351 - INFO - Summarizing 1 texts\n",
      "2024-11-02 03:01:18,299 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:18,304 - INFO - Summarizing 2 texts\n",
      "2024-11-02 03:01:20,442 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:20,446 - INFO - Summarizing 5 texts\n",
      "2024-11-02 03:01:22,880 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:22,887 - INFO - Summarizing 2 texts\n",
      "2024-11-02 03:01:24,850 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:24,855 - INFO - Processing level 2\n",
      "2024-11-02 03:01:24,856 - INFO - Embedding 10 texts\n",
      "2024-11-02 03:01:25,464 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:26,056 - INFO - Performing clustering with 5 clusters\n",
      "2024-11-02 03:01:28,049 - INFO - Summarizing 2 texts\n",
      "2024-11-02 03:01:29,963 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:29,968 - INFO - Summarizing 3 texts\n",
      "2024-11-02 03:01:32,483 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:32,488 - INFO - Summarizing 1 texts\n",
      "2024-11-02 03:01:35,482 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:35,488 - INFO - Summarizing 3 texts\n",
      "2024-11-02 03:01:36,939 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:36,943 - INFO - Summarizing 1 texts\n",
      "2024-11-02 03:01:38,333 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:38,336 - INFO - Processing level 3\n",
      "2024-11-02 03:01:38,337 - INFO - Embedding 5 texts\n",
      "2024-11-02 03:01:38,877 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:39,376 - INFO - Performing clustering with 2 clusters\n",
      "2024-11-02 03:01:40,482 - INFO - Summarizing 2 texts\n",
      "2024-11-02 03:01:42,585 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:01:42,589 - INFO - Summarizing 3 texts\n",
      "2024-11-02 03:01:44,125 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Build the RAPTOR tree\n",
    "tree_results = build_raptor_tree(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 03:07:43,281 - INFO - Building vectorstore with 48 texts\n",
      "2024-11-02 03:07:44,538 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:45,047 - INFO - Loading faiss.\n",
      "2024-11-02 03:07:45,093 - INFO - Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "# Build vectorstore\n",
    "vectorstore = build_vectorstore(tree_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 03:07:45,110 - INFO - Creating contextual compression retriever\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = create_retriever(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a query and observe where it got the data from + results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 03:07:45,119 - INFO - Processing query: What is the greenhouse effect?\n",
      "/var/folders/gl/6qlsk6x94klgs54qzsg9q1jm0000gn/T/ipykernel_8259/3372560863.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  level_docs = retriever.get_relevant_documents(\n",
      "2024-11-02 03:07:45,869 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:46,721 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:47,713 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:51,049 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:52,321 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:53,148 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:54,265 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:56,108 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:57,134 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:58,773 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:07:59,698 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:08:00,923 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:08:01,961 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:08:03,910 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:08:06,929 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:08:07,552 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:08:09,238 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:08:10,883 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:08:11,982 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:08:13,332 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "/var/folders/gl/6qlsk6x94klgs54qzsg9q1jm0000gn/T/ipykernel_8259/2783576077.py:25: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n",
      "/var/folders/gl/6qlsk6x94klgs54qzsg9q1jm0000gn/T/ipykernel_8259/2783576077.py:26: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = chain.run(context=context, question=query)\n",
      "2024-11-02 03:08:15,038 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-02 03:08:15,042 - INFO - Query processing completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the greenhouse effect?\n",
      "\n",
      "Number of documents retrieved: 16\n",
      "\n",
      "Retrieved Documents:\n",
      "  Document 1:\n",
      "    Content: The greenhouse effect is caused by greenhouse gases, such as carbon dioxide, methane, and nitrous ox...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 1\n",
      "    Origin: summary_of_cluster_8_level_0\n",
      "\n",
      "  Document 2:\n",
      "    Content: The provided context does not contain information about the greenhouse effect. It primarily discusse...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 3:\n",
      "    Content: The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), met...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 4:\n",
      "    Content: The context does not explicitly define the greenhouse effect. However, it mentions greenhouse gas em...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 2\n",
      "    Origin: summary_of_cluster_2_level_1\n",
      "\n",
      "  Document 5:\n",
      "    Content: The greenhouse effect refers to the process by which greenhouse gases, such as carbon dioxide, metha...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 1\n",
      "    Origin: summary_of_cluster_8_level_0\n",
      "\n",
      "  Document 6:\n",
      "    Content: The greenhouse effect is the process by which greenhouse gases, such as carbon dioxide (CO2), methan...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 7:\n",
      "    Content: The context does not provide a definition of the greenhouse effect. It discusses climate change, its...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 2\n",
      "    Origin: summary_of_cluster_2_level_1\n",
      "\n",
      "  Document 8:\n",
      "    Content: The provided context does not explicitly define the greenhouse effect. However, it discusses greenho...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 9:\n",
      "    Content: The greenhouse effect is caused by greenhouse gases, such as carbon dioxide, methane, and nitrous ox...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 1\n",
      "    Origin: summary_of_cluster_8_level_0\n",
      "\n",
      "  Document 10:\n",
      "    Content: The provided context does not contain information specifically about the greenhouse effect. It prima...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 2\n",
      "    Origin: summary_of_cluster_2_level_1\n",
      "\n",
      "  Document 11:\n",
      "    Content: The provided context does not include information about the greenhouse effect. It focuses on strateg...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 1\n",
      "    Origin: summary_of_cluster_2_level_0\n",
      "\n",
      "  Document 12:\n",
      "    Content: The greenhouse effect is the process by which greenhouse gases, such as carbon dioxide (CO2), methan...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 13:\n",
      "    Content: The greenhouse effect refers to the process by which greenhouse gases, such as carbon dioxide, metha...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 1\n",
      "    Origin: summary_of_cluster_8_level_0\n",
      "\n",
      "  Document 14:\n",
      "    Content: The context provided does not specifically define the greenhouse effect. It discusses climate change...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 2\n",
      "    Origin: summary_of_cluster_2_level_1\n",
      "\n",
      "  Document 15:\n",
      "    Content: The provided context does not contain information specifically about the greenhouse effect. It focus...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 1\n",
      "    Origin: summary_of_cluster_2_level_0\n",
      "\n",
      "  Document 16:\n",
      "    Content: The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), met...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "\n",
      "Context used for answer generation:\n",
      "The greenhouse effect is caused by greenhouse gases, such as carbon dioxide, methane, and nitrous oxide, which trap heat in the atmosphere and contribute to a warming climate.\n",
      "\n",
      "The provided context does not contain information about the greenhouse effect. It primarily discusses methane and nitrous oxide emissions from agriculture, the effects of climate change, and related environmental issues.\n",
      "\n",
      "The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, keeping the planet warm enough to support life. However, human activities have intensified this process, leading to a warmer climate.\n",
      "\n",
      "The context does not explicitly define the greenhouse effect. However, it mentions greenhouse gas emissions, such as carbon dioxide, methane, and nitrous oxide, which are linked to climate change. The greenhouse effect generally refers to the warming of the Earth's surface due to the trapping of heat by these gases in the atmosphere.\n",
      "\n",
      "The greenhouse effect refers to the process by which greenhouse gases, such as carbon dioxide, methane, and nitrous oxide, trap heat in the atmosphere, leading to a warming climate. This effect is largely driven by human activities, particularly fossil fuel consumption.\n",
      "\n",
      "The greenhouse effect is the process by which greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun in the atmosphere. This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
      "\n",
      "The context does not provide a definition of the greenhouse effect. It discusses climate change, its causes, and consequences but does not specifically mention the greenhouse effect itself.\n",
      "\n",
      "The provided context does not explicitly define the greenhouse effect. However, it discusses greenhouse gas emissions such as methane and nitrous oxide related to agriculture and climate change effects, indicating the role of these gases in contributing to global warming. To answer the question accurately, the greenhouse effect can be summarized as the warming of Earth's surface due to the trapping of heat by greenhouse gases in the atmosphere.\n",
      "\n",
      "The greenhouse effect is caused by greenhouse gases, such as carbon dioxide, methane, and nitrous oxide, which trap heat in the atmosphere and contribute to a warming climate. This effect is largely driven by human activities, particularly fossil fuel consumption.\n",
      "\n",
      "The provided context does not contain information specifically about the greenhouse effect. It primarily discusses climate change, its causes, and consequences. Therefore, there is no relevant information extracted for answering the question about the greenhouse effect.\n",
      "\n",
      "The provided context does not include information about the greenhouse effect. It focuses on strategies for enhancing energy efficiency, reducing emissions, and promoting sustainability. Therefore, there is no relevant information to extract regarding the greenhouse effect.\n",
      "\n",
      "The greenhouse effect is the process by which greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun in the atmosphere. This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
      "\n",
      "The greenhouse effect refers to the process by which greenhouse gases, such as carbon dioxide, methane, and nitrous oxide, trap heat in the Earth's atmosphere, contributing to a warming climate. This effect is largely driven by human activities, particularly the combustion of fossil fuels.\n",
      "\n",
      "The context provided does not specifically define the greenhouse effect. It discusses climate change and its causes, such as greenhouse gas emissions (notably carbon dioxide, methane, and nitrous oxide), but does not elaborate on the greenhouse effect itself. Therefore, there is no relevant information extracted for answering the question about the greenhouse effect.\n",
      "\n",
      "The provided context does not contain information specifically about the greenhouse effect. It focuses on strategies for enhancing energy efficiency, reducing emissions, and promoting sustainability.\n",
      "\n",
      "The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, keeping the planet warm enough to support life. However, human activities have intensified this process, leading to a warmer climate.\n",
      "\n",
      "Generated Answer:\n",
      "The greenhouse effect is the process by which greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun in the Earth's atmosphere. This natural phenomenon is essential for maintaining the planet's temperature, allowing it to support life. However, human activities, particularly the burning of fossil fuels, have intensified this effect, leading to a warming climate and contributing to global climate change.\n",
      "\n",
      "Model Used: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "max_level = 3  # Adjust based on your tree depth\n",
    "query = \"What is the greenhouse effect?\"\n",
    "result = raptor_query(query, retriever, max_level)\n",
    "print_query_details(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
