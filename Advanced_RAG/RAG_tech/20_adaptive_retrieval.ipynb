{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 적응형 Retrieval-Augmented Generation (RAG) 시스템\n",
    "\n",
    "## 개요\n",
    "\n",
    "이 시스템은 다양한 쿼리 유형에 맞춰 검색 전략을 조정하는 고급 RAG(정보 검색 증강 생성) 접근 방식을 구현합니다. 여러 단계에서 대형 언어 모델(LLM)을 활용하여 사용자 쿼리에 대해 더 정확하고 관련성 높은, 그리고 상황에 맞는 응답을 제공하는 것을 목표로 합니다.\n",
    "\n",
    "## 동기\n",
    "\n",
    "기존의 RAG 시스템은 검색에 일관된 접근 방식을 취하는 경우가 많지만, 이는 다양한 쿼리 유형에 대해 비효율적일 수 있습니다. 본 적응형 시스템은 다양한 질문이 서로 다른 검색 전략을 필요로 한다는 이해에서 출발했습니다. 예를 들어, 사실 기반 쿼리는 정확하고 집중적인 검색이 효과적일 수 있지만, 분석적인 쿼리는 더 광범위하고 다양한 정보가 필요할 수 있습니다.\n",
    "\n",
    "## 주요 구성 요소\n",
    "\n",
    "1. **쿼리 분류기**: 쿼리 유형을 결정 (사실 기반, 분석적, 의견, 상황 기반)\n",
    "2. **적응형 검색 전략**: 서로 다른 쿼리 유형에 맞춘 네 가지 검색 전략\n",
    "   - 사실 기반 전략\n",
    "   - 분석적 전략\n",
    "   - 의견 전략\n",
    "   - 상황 기반 전략\n",
    "3. **LLM 통합**: 검색 및 순위 매김 과정에서 LLM을 활용\n",
    "4. **OpenAI GPT 모델**: 최종 응답 생성을 위해 검색된 문서를 컨텍스트로 사용\n",
    "\n",
    "## 방법 세부사항\n",
    "\n",
    "### 1. 쿼리 분류\n",
    "\n",
    "시스템은 우선 사용자의 쿼리를 다음 네 가지 유형 중 하나로 분류합니다:\n",
    "- **사실 기반**: 구체적이고 검증 가능한 정보를 찾는 질문\n",
    "- **분석적**: 포괄적 분석이나 설명을 필요로 하는 질문\n",
    "- **의견**: 주관적인 사안이나 다양한 관점을 찾는 질문\n",
    "- **상황 기반**: 사용자 특정 맥락에 의존하는 질문\n",
    "\n",
    "### 2. 적응형 검색 전략\n",
    "\n",
    "각 쿼리 유형에 따라 고유한 검색 전략이 실행됩니다:\n",
    "\n",
    "#### 사실 기반 전략\n",
    "- 정확성을 높이기 위해 LLM을 활용하여 쿼리를 개선합니다.\n",
    "- 개선된 쿼리를 바탕으로 문서를 검색합니다.\n",
    "- 검색된 문서를 LLM을 이용해 관련성에 따라 순위를 매깁니다.\n",
    "\n",
    "#### 분석적 전략\n",
    "- LLM을 사용하여 주요 쿼리에 대해 다양한 측면을 다룰 수 있도록 여러 하위 쿼리를 생성합니다.\n",
    "- 각 하위 쿼리에 대해 문서를 검색합니다.\n",
    "- 검색된 문서의 다양성을 유지하면서 최종 문서를 선정하도록 LLM을 활용합니다.\n",
    "\n",
    "#### 의견 전략\n",
    "- LLM을 사용하여 주제에 대한 다양한 관점을 식별합니다.\n",
    "- 각 관점을 대표하는 문서를 검색합니다.\n",
    "- 검색된 문서들 중 다양한 의견을 포함할 수 있도록 선택하여 제공합니다.\n",
    "\n",
    "#### 상황 기반 전략\n",
    "- LLM을 사용하여 사용자 특정 맥락을 쿼리에 포함시킵니다.\n",
    "- 맥락화된 쿼리에 따라 문서를 검색합니다.\n",
    "- 사용자 맥락을 고려하여 관련성과 맥락적 적합성에 따라 문서를 순위 매깁니다.\n",
    "\n",
    "### 3. LLM 기반 순위 매김\n",
    "\n",
    "검색 후 각 전략은 최종 문서 순위를 매기기 위해 LLM을 사용합니다. 이 단계는 가장 관련성 높고 적합한 문서들이 다음 단계로 넘어가도록 보장합니다.\n",
    "\n",
    "### 4. 응답 생성\n",
    "\n",
    "최종 검색된 문서들은 OpenAI GPT 모델로 전달되며, 이 모델은 쿼리와 제공된 컨텍스트를 기반으로 응답을 생성합니다.\n",
    "\n",
    "## 접근 방식의 이점\n",
    "\n",
    "1. **정확성 향상**: 쿼리 유형에 맞는 검색 전략을 통해 더 정확하고 관련성 높은 정보를 제공합니다.\n",
    "2. **유연성**: 시스템이 다양한 쿼리 유형에 맞게 적응하여 광범위한 사용자 요구를 처리할 수 있습니다.\n",
    "3. **상황 인식**: 특히 상황 기반 쿼리의 경우, 사용자 특정 정보를 통합하여 더 개인화된 응답을 제공합니다.\n",
    "4. **다양한 관점 제공**: 의견 기반 쿼리의 경우, 시스템이 여러 관점을 탐색하고 제시합니다.\n",
    "5. **포괄적 분석**: 분석적 전략을 통해 복잡한 주제에 대해 철저한 탐색을 수행합니다.\n",
    "\n",
    "## 결론\n",
    "\n",
    "본 적응형 RAG 시스템은 기존 RAG 접근 방식에 비해 큰 진전을 나타냅니다. 검색 전략을 동적으로 조정하고 과정 전반에 걸쳐 LLM을 활용함으로써, 다양한 사용자 쿼리에 대해 더 정확하고, 관련성 높은, 그리고 세밀한 응답을 제공하는 것을 목표로 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/adaptive_retrieval.svg\" alt=\"adaptive retrieval\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import Dict, Any\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "from helper_functions import *\n",
    "from evaluation.evalute_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the query classifer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class categories_options(BaseModel):\n",
    "        category: str = Field(description=\"The category of the query, the options are: Factual, Analytical, Opinion, or Contextual\", example=\"Factual\")\n",
    "\n",
    "\n",
    "class QueryClassifier:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Classify the following query into one of these categories: Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nCategory:\"\n",
    "        )\n",
    "        self.chain = self.prompt | self.llm.with_structured_output(categories_options)\n",
    "\n",
    "\n",
    "    def classify(self, query):\n",
    "        print(\"clasiffying query\")\n",
    "        return self.chain.invoke(query).category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Base Retriever class, such that the complex ones will inherit from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRetrievalStrategy:\n",
    "    def __init__(self, texts):\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "        self.documents = text_splitter.create_documents(texts)\n",
    "        self.db = FAISS.from_documents(self.documents, self.embeddings)\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "\n",
    "\n",
    "    def retrieve(self, query, k=4):\n",
    "        return self.db.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Factual retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relevant_score(BaseModel):\n",
    "        score: float = Field(description=\"The relevance score of the document to the query\", example=8.0)\n",
    "\n",
    "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4):\n",
    "        print(\"retrieving factual\")\n",
    "        # Use LLM to enhance the query -> 더 나은 정보 반환을 위한 사실적 쿼리로 향상시킴 \n",
    "        enhanced_query_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Enhance this factual query for better information retrieval: {query}\"\n",
    "        )\n",
    "        query_chain = enhanced_query_prompt | self.llm\n",
    "        enhanced_query = query_chain.invoke(query).content\n",
    "        print(f'enhande query: {enhanced_query}')\n",
    "\n",
    "        # 새로운 쿼리로 유사도검색 \n",
    "        docs = self.db.similarity_search(enhanced_query, k=k*2)\n",
    "\n",
    "        # llm으로 reranking 함 \n",
    "        ranking_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"doc\"],\n",
    "            template=\"On a scale of 1-10, how relevant is this document to the query: '{query}'?\\nDocument: {doc}\\nRelevance score:\"\n",
    "        )\n",
    "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
    "\n",
    "        ranked_docs = []\n",
    "        print(\"ranking docs\")\n",
    "        for doc in docs:\n",
    "            input_data = {\"query\": enhanced_query, \"doc\": doc.page_content}\n",
    "            score = float(ranking_chain.invoke(input_data).score)\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "        # Sort by relevance score and return top k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in ranked_docs[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Analytical reriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectedIndices(BaseModel):\n",
    "    indices: List[int] = Field(description=\"Indices of selected documents\", example=[0, 1, 2, 3])\n",
    "\n",
    "class SubQueries(BaseModel):\n",
    "    sub_queries: List[str] = Field(description=\"List of sub-queries for comprehensive analysis\", example=[\"What is the population of New York?\", \"What is the GDP of New York?\"])\n",
    "\n",
    "class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4):\n",
    "        print(\"retrieving analytical\")\n",
    "        # 분석적 질문이므로 쿼리를 서브 쿼리로 나누는 과정이 필요함 \n",
    "        sub_queries_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Generate {k} sub-questions for: {query}\"\n",
    "        )\n",
    "\n",
    "        llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "        sub_queries_chain = sub_queries_prompt | llm.with_structured_output(SubQueries)\n",
    "\n",
    "        input_data = {\"query\": query, \"k\": k}\n",
    "        sub_queries = sub_queries_chain.invoke(input_data).sub_queries\n",
    "        print(f'sub queries for comprehensive analysis: {sub_queries}')\n",
    "\n",
    "        all_docs = []\n",
    "        # 하위 질문에 대해 2가지씩 답변 \n",
    "        for sub_query in sub_queries:\n",
    "            all_docs.extend(self.db.similarity_search(sub_query, k=2))\n",
    "\n",
    "        # 다양하고 관련있는 문서들을 뽑아냄 \n",
    "        diversity_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"docs\", \"k\"],\n",
    "            template=\"\"\"Select the most diverse and relevant set of {k} documents for the query: '{query}'\\nDocuments: {docs}\\n\n",
    "            Return only the indices of selected documents as a list of integers.\"\"\"\n",
    "        )\n",
    "        diversity_chain = diversity_prompt | self.llm.with_structured_output(SelectedIndices)\n",
    "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:50]}...\" for i, doc in enumerate(all_docs)])\n",
    "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
    "        selected_indices_result = diversity_chain.invoke(input_data).indices\n",
    "        print(f'selected diverse and relevant documents')\n",
    "        \n",
    "        return [all_docs[i] for i in selected_indices_result if i < len(all_docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Opinion retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpinionRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=3):\n",
    "        print(\"retrieving opinion\")\n",
    "        # llm을 통해 다양한 주제에 대한 관점을 먼저 식별함 \n",
    "        viewpoints_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Identify {k} distinct viewpoints or perspectives on the topic: {query}\"\n",
    "        )\n",
    "        viewpoints_chain = viewpoints_prompt | self.llm\n",
    "        input_data = {\"query\": query, \"k\": k}\n",
    "        viewpoints = viewpoints_chain.invoke(input_data).content.split('\\n')\n",
    "        print(f'viewpoints: {viewpoints}')\n",
    "\n",
    "        all_docs = []\n",
    "        for viewpoint in viewpoints:\n",
    "            all_docs.extend(self.db.similarity_search(f\"{query} {viewpoint}\", k=2))\n",
    "\n",
    "        # 다양한 관점을 사용하요 문서를 분류하고, 대표적인 문서를 선택함 \n",
    "        opinion_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"docs\", \"k\"],\n",
    "            template=\"Classify these documents into distinct opinions on '{query}' and select the {k} most representative and diverse viewpoints:\\nDocuments: {docs}\\nSelected indices:\"\n",
    "        )\n",
    "        opinion_chain = opinion_prompt | self.llm.with_structured_output(SelectedIndices)\n",
    "        \n",
    "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:100]}...\" for i, doc in enumerate(all_docs)])\n",
    "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
    "        selected_indices = opinion_chain.invoke(input_data).indices\n",
    "        print(f'selected diverse and relevant documents')\n",
    "        \n",
    "        return [all_docs[int(i)] for i in selected_indices.split() if i.isdigit() and int(i) < len(all_docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Contextual retriever strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4, user_context=None):\n",
    "        print(\"retrieving contextual\")\n",
    "        # 사용자가 제공한 배경 정보를 쿼리에 포함하게 함 -> 사용자 쿼리의 맥락을 이해하고 사용자의 요구에 맞게 쿼리를 재구성함 \n",
    "        context_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\"],\n",
    "            template=\"Given the user context: {context}\\nReformulate the query to best address the user's needs: {query}\"\n",
    "        )\n",
    "        context_chain = context_prompt | self.llm\n",
    "        input_data = {\"query\": query, \"context\": user_context or \"No specific context provided\"}\n",
    "        contextualized_query = context_chain.invoke(input_data).content\n",
    "        print(f'contextualized query: {contextualized_query}')\n",
    "\n",
    "        # 재구성된 쿼리로 k의 제곱만큼 뽑아냄 \n",
    "        docs = self.db.similarity_search(contextualized_query, k=k*2)\n",
    "\n",
    "        # 문서와 쿼리의 관련성을 점수로 평가하여 정렬한다. \n",
    "        ranking_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\", \"doc\"],\n",
    "            template=\"Given the query: '{query}' and user context: '{context}', rate the relevance of this document on a scale of 1-10:\\nDocument: {doc}\\nRelevance score:\"\n",
    "        )\n",
    "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
    "        print(\"ranking docs\")\n",
    "\n",
    "        ranked_docs = []\n",
    "        for doc in docs:\n",
    "            input_data = {\"query\": contextualized_query, \"context\": user_context or \"No specific context provided\", \"doc\": doc.page_content}\n",
    "            score = float(ranking_chain.invoke(input_data).score)\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "\n",
    "        # Sort by relevance score and return top k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return [doc for doc, _ in ranked_docs[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Adapive retriever class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRetriever:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.classifier = QueryClassifier()\n",
    "        self.strategies = {\n",
    "            \"Factual\": FactualRetrievalStrategy(texts),\n",
    "            \"Analytical\": AnalyticalRetrievalStrategy(texts),\n",
    "            \"Opinion\": OpinionRetrievalStrategy(texts),\n",
    "            \"Contextual\": ContextualRetrievalStrategy(texts)\n",
    "        }\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        category = self.classifier.classify(query)\n",
    "        strategy = self.strategies[category]\n",
    "        return strategy.retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define aditional retriever that inherits from langchain BaseRetriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PydanticAdaptiveRetriever(BaseRetriever):\n",
    "    adaptive_retriever: AdaptiveRetriever = Field(exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.adaptive_retriever.get_relevant_documents(query)\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Adaptive RAG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRAG:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        adaptive_retriever = AdaptiveRetriever(texts)\n",
    "        self.retriever = PydanticAdaptiveRetriever(adaptive_retriever=adaptive_retriever)\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "        \n",
    "        # Create a custom prompt\n",
    "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        \n",
    "        # Create the LLM chain\n",
    "        self.llm_chain = prompt | self.llm\n",
    "        \n",
    "      \n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        # 어떤 질문인지 분류함 -> 분류에 맞게 카테고리와 전략을 추출 -> 이에 맞게 수정한 후 관련 문서를 추출함 \n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        input_data = {\"context\": \"\\n\".join([doc.page_content for doc in docs]), \"question\": query}\n",
    "        return self.llm_chain.invoke(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate use of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "texts = [\n",
    "    \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\"\n",
    "    ]\n",
    "rag_system = AdaptiveRAG(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showcase the four different types of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_result = rag_system.answer(\"What is the distance between the Earth and the Sun?\").content\n",
    "print(f\"Answer: {factual_result}\")\n",
    "\n",
    "analytical_result = rag_system.answer(\"How does the Earth's distance from the Sun affect its climate?\").content\n",
    "print(f\"Answer: {analytical_result}\")\n",
    "\n",
    "opinion_result = rag_system.answer(\"What are the different theories about the origin of life on Earth?\").content\n",
    "print(f\"Answer: {opinion_result}\")\n",
    "\n",
    "contextual_result = rag_system.answer(\"How does the Earth's position in the Solar System influence its habitability?\").content\n",
    "print(f\"Answer: {contextual_result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
