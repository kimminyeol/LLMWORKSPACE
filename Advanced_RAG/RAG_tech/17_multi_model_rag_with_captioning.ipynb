{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개요\n",
    "멀티모달 RAG 시스템 구현 -> PDF에서 텍스트와 이미지를 추출하고, 이를 멀티모달 RAG 시스템을 통해 요약 및 검색하여 질문 응답을 제공\n",
    "\n",
    "### 주요 구성 요소\n",
    "- **PyMuPDF**: PDF에서 텍스트와 이미지를 추출하는 데 사용\n",
    "- **Gemini 1.5-flash 모델**: 이미지와 표를 요약하는 데 사용\n",
    "- **Cohere Embeddings**: 문서의 청크를 임베딩하는 데 사용\n",
    "- **Chroma Vectorstore**: 문서 임베딩을 저장하고 검색하는 벡터 스토어\n",
    "- **LangChain**: 검색 및 생성 파이프라인을 조정\n",
    "\n",
    "### 다이어그램\n",
    "   <img src=\"../images/multi_model_rag_with_captioning.svg\" alt=\"Reliable-RAG\" width=\"300\">\n",
    "\n",
    "### 동기\n",
    "복잡한 문서를 효율적으로 요약하여 멀티모달 데이터를 쉽게 검색하고 간결하게 응답을 제공하는 것이 이 프로젝트의 목표입니다.\n",
    "\n",
    "### 방법 설명\n",
    "1. **텍스트 및 이미지 추출**: PyMuPDF를 사용하여 PDF에서 텍스트와 이미지를 추출합니다.\n",
    "2. **요약**: 추출된 이미지와 표를 Gemini 모델을 사용하여 요약합니다.\n",
    "3. **임베딩 생성**: Cohere를 통해 생성된 임베딩을 Chroma에 저장합니다.\n",
    "4. **검색**: 쿼리를 기반으로 유사도 기반 검색기를 사용하여 관련 섹션을 가져옵니다.\n",
    "\n",
    "### 이점\n",
    "- 복잡하고 멀티모달로 구성된 문서에서 간단한 검색과 요약이 가능합니다.\n",
    "- 텍스트와 이미지 모두에 대한 Q&A 과정이 간소화됩니다.\n",
    "- 다른 문서 유형으로 확장이 용이한 유연한 구조입니다.\n",
    "\n",
    "### 구현\n",
    "1. **문서 청크 분할**: 텍스트 스플리터를 사용해 문서를 청크 단위로 겹쳐서 분할합니다.\n",
    "2. **요약된 텍스트 및 이미지 콘텐츠 벡터화**: 요약된 텍스트와 이미지 콘텐츠를 벡터로 저장합니다.\n",
    "3. **쿼리 처리**: 관련 문서 세그먼트를 검색하고 간결한 답변을 생성하여 응답합니다.\n",
    "\n",
    "### 요약\n",
    "이 프로젝트는 멀티모달 문서의 처리와 검색을 가능하게 하여, 최신 LLM과 벡터 기반 검색 시스템을 결합함으로써 간결하고 관련성 높은 응답을 제공합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kmy_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install google-generativeai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the \"Attention is all you need\" paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2163k  100 2163k    0     0  1068k      0  0:00:02  0:00:02 --:--:-- 1068k\n",
      "mv: 1706.03762: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! curl -o attention_is_all_you_need.pdf https://arxiv.org/pdf/1706.03762\n",
    "! mv 1706.03762 attention_is_all_you_need.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_is_all_you_need.pdf\n"
     ]
    }
   ],
   "source": [
    "# ! wget https://arxiv.org/pdf/1706.03762\n",
    "# ! mv 1706.03762 attention_is_all_you_need.pdf\n",
    "! ls | grep attention_is_all_you_need.pdf\n",
    "\n",
    "# ! ls | x.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "img_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fitz.open('y.pdf') as pdf_file:\n",
    "    # 이미지를 저장할 디렉토리가 없으면 새로 생성\n",
    "    if not os.path.exists(\"extracted_images\"):\n",
    "        os.makedirs(\"extracted_images\")\n",
    "\n",
    "    # PDF 파일의 각 페이지를 순회하며 처리\n",
    "    for page_number in range(len(pdf_file)):\n",
    "        page = pdf_file[page_number]  # 현재 페이지 가져오기\n",
    "        \n",
    "        # 페이지의 텍스트 추출 및 정리\n",
    "        text = page.get_text().strip()\n",
    "        # 페이지 번호와 텍스트를 딕셔너리 형태로 저장\n",
    "        text_data.append({\"response\": text, \"name\": page_number + 1})\n",
    "        \n",
    "        # 현재 페이지에서 발견된 이미지 목록 가져오기\n",
    "        images = page.get_images(full=True)\n",
    "\n",
    "        # 페이지에서 찾은 모든 이미지를 순회하며 처리\n",
    "        for image_index, img in enumerate(images, start=0):\n",
    "            xref = img[0]  # 이미지의 XREF(참조 ID) 가져오기\n",
    "            base_image = pdf_file.extract_image(xref)  # XREF를 사용해 이미지 추출\n",
    "            image_bytes = base_image[\"image\"]  # 추출한 이미지의 바이트 데이터\n",
    "            image_ext = base_image[\"ext\"]  # 이미지 파일 확장자 정보\n",
    "            \n",
    "            # 이미지 바이트 데이터를 PIL로 읽어와 이미지 파일로 저장\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            # 이미지 파일명에 페이지 번호와 이미지 인덱스 포함\n",
    "            image.save(f\"extracted_images/image_{page_number+1}_{image_index+1}.{image_ext}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(\"extracted_images\"):\n",
    "    # \"extracted_images\" 폴더에 있는 모든 이미지 파일을 순회합니다.\n",
    "    image = Image.open(f\"extracted_images/{img}\")  # 현재 이미지 파일을 열어 Image 객체로 만듭니다.\n",
    "    \n",
    "    # 이미지와 요약 요청을 함께 전송하여 요약 생성 -> 이미지 to text하는 모델 \n",
    "    response = model.generate_content([\n",
    "        image,\n",
    "        \"You are an assistant tasked with summarizing tables, images and text for retrieval. \"\n",
    "        \"These summaries will be embedded and used to retrieve the raw text or table elements. \"\n",
    "        \"Give a concise summary of the table or text that is well optimized for retrieval. Table or text or image:\"\n",
    "    ])\n",
    "    # response 객체에서 요약된 텍스트를 가져와 img_data 리스트에 저장\n",
    "    img_data.append({\n",
    "        \"response\": response.text,  # 생성된 요약 텍스트\n",
    "        \"name\": img  # 원본 이미지 파일 이름\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'response': 'The table shows the prices of different types of 우도 민도 (Udo Mind) products. The prices are listed in Korean won. The types of Udo Mind products are: \\n- \\n- \\n- \\n-  . The table also notes that the prices may vary depending on the size and type of product.',\n",
       "  'name': 'image_1_1.jpeg'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectostore\n",
    "- cohere embedding을 통해 추출한 이미지와 논문 내용을 임베딩하고 split하여 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embeddings\n",
    "embedding_model = CohereEmbeddings(model=\"embed-english-v3.0\", cohere_api_key=\"ew8pvCv9N3kLp07D1SXJHS1dH8DU13QQGsls7N7o\")\n",
    "\n",
    "# Load the document\n",
    "docs_list = [Document(page_content=text['response'], metadata={\"name\": text['name']}) for text in text_data]\n",
    "img_list = [Document(page_content=img['response'], metadata={\"name\": img['name']}) for img in img_data]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=400, chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "img_splits = text_splitter.split_documents(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits + img_splits, # adding the both text and image splits\n",
    "    collection_name=\"multi_model_rag\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 1}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the BLEU score of the Transformer (base model)?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer (base model) achieves a BLEU score of 27.3.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. \n",
    "Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(model=\"command-r-plus\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\":docs[0].page_content, \"question\": query})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
